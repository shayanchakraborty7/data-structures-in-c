Quicksort:

Best case: O(n log n)
Average case: O(n log n)
Worst case: O(n^2)

Mergesort:

Best case: O(n log n)
Average case: O(n log n)
Worst case: O(n log n)

Binary search:

Best case: O(1)
Average case: O(log n)
Worst case: O(log n)

Linear search:

Best case: O(1)
Average case: O(n)
Worst case: O(n)

Selection sort:

Best case: O(n^2)
Average case: O(n^2)
Worst case: O(n^2)

Heap sort:

Best case: O(n log n)
Average case: O(n log n)
Worst case: O(n log n)

Bubble sort:

Best case: O(n)
Average case: O(n^2)
Worst case: O(n^2)

Insertion sort:

Best case: O(n)
Average case: O(n^2)
Worst case: O(n^2)

0/1 knapsack problem:

The time complexity depends on the algorithm used to solve it. Dynamic programming solutions typically have a time complexity of O(nW), where n is the number of items and W is the capacity of the knapsack.

Fractional knapsack problem:

Typically solved in O(n log n) using sorting algorithms.

Dijkstra's algorithm:

Best case: O((V + E) log V) with a binary heap
Average case: O((V + E) log V) with a binary heap
Worst case: O((V^2) log V) with a binary heap

Floyd Warshall algorithm:

Time complexity: O(V^3), where V is the number of vertices.

Bellman Ford algorithm:

Best case: O(VE)
Average case: O(VE)
Worst case: O(VE)

Kruskal's algorithm:

Time complexity: O(E log E) or O(E log V), where E is the number of edges and V is the number of vertices.

Prim's algorithm:

Best case: O(E + V log V)
Average case: O(E + V log V)
Worst case: O(E + V log V)

Strassen's matrix multiplication:

Time complexity: O(n^2.81)

Travelling Salesman Problem (TSP):

The exact solution is typically solved using dynamic programming or branch and bound techniques with a time complexity of O(n^2 * 2^n), where n is the number of cities. Approximation algorithms may have different time complexities.

Job sequencing problem:

Time complexity varies depending on the approach used. Typically, it can be solved in O(n log n) using greedy algorithms, sorting by decreasing order of profit or deadlines.